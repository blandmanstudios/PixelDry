python3 -m venv env

source env/bin/activate

python3 -m pip install requests

./scape.py

# created an app in discord called watching-computers-draw

python3 -m pip install pyyaml

# find does a requets then sleeps for a seconds. It has an outer loop
# the outer loop max is the max runtime, set this to 60 * 60 to run for an hour

# track has an outer loop with a sleep for one second. while its waiting for
# work to do. The actual work it does is on the inner loop where it will do a
# requets at each work item it pops off the  list and its sleeps after each work
# item
# So what i want to do is give it a LIMIT 1 on the work items it grabs so it
# only ever does one work item this will rate limit me to one request per second
# and then i can set the outer loop as the max run time
# but i dont love that cuz then i might fall behind and miss something if we're
# rendering multiple images at the same time
# i meant thats the case now cuz i have a rate limit in, so i might as well keep
# it that way
# i can add logic later for concurrency in a safe way

# then download will check once per second if there is anything to download then
# if there is it will try it all
# so its loop time is its run time too

# lets run them all for 200 seconds and see what happens
# we should run for 200 seconds then they all stop
# and we should be confident we're only gitting the discord api 3x per second

rm local_state.db
./find_new_renders.py
./track_new_render.py
./download_images.py

# the 200 second test went well but i killed the results not sure why
# i also learned i need to be able to handle content that has apostophes and
# not have it break my sqlite

# running a 60 second test now

# use the following in a sqlite shell to track progress
select count() from beginnings; select count() from progressions; select count() from endings;
# it looks liek this
sqlite> select count() from beginnings; select count() from progressions; select count() from endings;
2
4
0


# a 200 run looked like this
sqlite> select count() from beginnings; select count() from progressions; select count() from endings;
3
14
1
# it got 1 full render, the other i think we missed the output cuz it fell out
# of the top 10, i'll change that to top 50


# i had a good 10 minute run 600 iterations (tho it took longer than 10 minutes)
# and it reutrned this
sqlite> select count() from beginnings; select count() from progressions; select count() from endings;
15
69
9
# so i can get 9 full images beging drawn which is probably about a minute
# of content
# so right now we dont have enough data coming in for a real time no repeats
# stream, but when we jump up to 3 channels and include upscalings and other
# types we might get there (and we can code it to work for things with https
# and apostophes


# lets try a 1 hour capture with 3 channels finding renders
# 1 tracker tracking renders
# and 1 downloader downloading images
# it should at most do 4 requets per second using my token, maybe even just 3
# and the downloads are also going to my ip address but thats just one per
# second
# lets see how far it gets before i stop it
# in an hour and 25 minutes it got this far that was awesome, i'll save it
# to cap6

# helpful command
./find_new_renders.py -c 24 & ./find_new_renders.py -c 54 & ./find_new_renders.py -c 84 &
# helpful command
kill %1 %2 %3

# letting it run for 4 hours, set each one to 60*60*4
# not even going to reset it from running the hour ealier
# it can just pickup where it left off (or give up on those)
# it worked, i got like 78 minutes of content in those 4 hours
# eventually mind find_new_renders threads died on your down cuz of a connection
# error with name resolution, so i could fix that or i could not worry about it
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='discord.com', port=443): Max retries exceeded with url: /api/v10/channels/997271660900126801/messages?limit=100 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fadffcaaa40>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))



Running a long test, on youtube unlisted, potentially overnight
Move state before to save_point3 so we know where we started
- we have secure params
- we have a clips_metadata.db with a library of clips all with zero playtime
- we have those clips saved in clips/
- we have nothing saved in data/ (it just has a .gitkeep file)
- we have no local_state.db database
Start a stream using youtube. Normal latency, enable DVR, default stream key
(RTMP, Variable), added delay = None
Run the following script on my desktop PC
./orchestrate.py
Looks to be working, but i just found that the bot has been intermittent today
(for the last 10 hours) so it looks like people arent actively using it
Maybe its time for me to just work in scrape only mode and build up a bigger
database of clips for when there are lulls in my livestream

Also, today I learned i've got to be on a wired connection if i'm going to
download images and upload a stream at the same time


I'm going to run
./orchestrate.py --query_only
And let it run for along time, and see if it collects a good buffer of stuff
  i can fallback on
Then, in the morning i can see how it ran, and i can run an orchestrate that
  streams and queries

It looks like it ran all night, the pids look differente so it looks like it
  did restart things in the middle of the night at least once, but i got a
  good buffer of images (1853 clips)
So this morning just before 12:06pm on November 11th, i started a
  ./orchestrate --stream_only and I'm going to let them run together and see
  if its successfull for 4+ hours and if there are any failures and if its
  good to go for a consistent number of time, I'll lauch it publicly tonight

Darn the stream broke 34 minutes in and started showing semi-gray frames
  sometimes, not a coincidence that this is just after the loop happened
  and it got ~100 clips in
  Seems to sometimes normal out then i get a funny clip again 
  It seems like its normaled out now that we're 30 minutes into the stream maybe
  nope it hasn't
  I wonder if it comes from the resolution changing from what it was last time
    ffmpeg sent out this clip....
  Yes! it is the ones with non-traditional resolution! it must be that its
    changing what ffmpeg remembered about that clip last time, i gott a reformat
    all my clips to be the same resolution (and while I'm at it I'll reformat
    them to be 1920x1080 so i have a high res stream when posible)
    And while i'm working on that fixing the images can be done with the same
      workflow

Next TODOs are
- The clip renderer should get a better query so it renders by timestamp so
  in the case where it falls behind (or if i gave it a big backlog to process)
  it will prioritize the late jobs (so those get put on the stream faster)
- Caution: i only have 60G of of space left and this project is already taking
  5G so i might have to clean off space or prep to move


Launching a potentially live stream 11/11 and 11:12PM

mkdir save_point8_launching_potentially_live_stream
cp clip_metadata.db save_point8_before_launching_potentially_live_stream/
cp local_state.db save_point8_before_launching_potentially_live_stream/
cp -r data save_point8_before_launching_potentially_live_stream/
cp -r clips save_point8_before_launching_potentially_live_stream/
then starting the stream on youtube
then running the following two commands in seperate terminals
./orchestrate.py --query_only >> orchestrate_query.log
./orchestrate.py --stream_only >> orchestrate_stream.log

There was one blip where the stream restarted in less than 30
  seconds, i happened to be watching at the time and it wasnt
  super disruptive
Sometime during the second hour one of the find scripts died
  and so did the track script. I have 20ish minutes left till
  it gets to a restart interval. I'm going to make sure it can
  restart those before I make the stream public. But once that
  works I can let this run knowing it will likely be able to
  go for many hours without me needing to watch it closely
At 3 hours they both restarted successfully as i can see with
  `ps aux | grep python` and i can see the number of tracked
  files increasing so we're back to working. I'm good to hit
  publish on the stream and see what happens
I hit the public button right as the stream ticked over to
  3 hours and 10 minutes

It stopped at like 4:45, did my computer reboot? what happened?
  I went back to the computer and it was logged out and no
  programs were open. I might need to script this as systemd
  services that start at boot

I started it up again, and its run for 11:38+
I started a backup stream with a specific watermark on my rasp
  pi and have it streaming at the backup url
I'm going to kill the main stream (./orchestrate.py --stream_only)
  and see if the rasp pi can take over
It worked!!!!
I didnt watch to see if a viewere would have been interrupted but
  on my streamer display it hung and put up a loading screen for
  a few seconds then it jumped over to the fallback stream
I restarted the main stream to see if it comes back as the primary
  but it doesnt look to be doing that automatically. I might need
  to temp kill the fallback stream to get youtube to failback to
  broadcasting the thing sent to the primary url
I'm going to do that. Killing the rpi stream temporarily by giving
  it a `killall ffmpeg` to see if it restarts itself and in the
  small downtime youtube will switch over to the healthy primary
  stream
Yes!! it switched back to the major stream
I can switch back and forth on purpose!
It takes about 10 seconds and i just need to kill the one i need to
  get off off
So now i can disconnect from my rpi stream and trust that if my
  computer reboots in the night the rpi will takeover as fallback
In the future i can automate even more insurance like having this
  start on my computers boot and run as a daemon process, but that
  isnt necessary yet. And i could have the same thing happen on the
  rpi but thats not necessary yet
